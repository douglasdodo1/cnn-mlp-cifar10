{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR10 with MLPs\n",
        "Simple starter notebook to benchmark your own MLP with PyTorch on the CIFAR-10 dataset.\n",
        "\n",
        "OBS.:\n",
        "\n",
        "- The main code is basically done, so focus on training the models and searching for the best hyperparameters and architectures.\n",
        "- You are not required to use this exact code or even the PyTorch library.\n",
        "- It is recommended to use execution environments with GPU access (such as Google Colab), since larger models will take more time to train.\n",
        "- Remember to document the history of your experiments and which results motivated the changes in subsequent experiments."
      ],
      "metadata": {
        "id": "BjND-9CP7R7n"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ErVptyZ7Npv"
      },
      "outputs": [],
      "source": [
        "#@title Libs\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Dataset Setup\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n"
      ],
      "metadata": {
        "id": "fcH-AHUK8eGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Defining the MLP model\n",
        "# 3072 (input) → 64 → 128 → 64 → 10 (output)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "  def __init__(self, input_size, num_classes, activation_function):\n",
        "    super(MLP,self).__init__()\n",
        "    # Defining activation functions and fully-connected layers\n",
        "    self.activation_function = activation_function\n",
        "    self.fc_input = nn.Linear(input_size, 64)\n",
        "    self.fc_hidden1 = nn.Linear(64, 128)\n",
        "    self.fc_hidden2 = nn.Linear(128, 64)\n",
        "    self.fc_output = nn.Linear(64, num_classes)\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.activation_function(self.fc_input(x))\n",
        "    x = self.activation_function(self.fc_hidden1(x))\n",
        "    x = self.activation_function(self.fc_hidden2(x))\n",
        "    x = self.fc_output(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "KLjycMiT8zo-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Defining metrics helper\n",
        "\n",
        "def get_scores(targets, predictions):\n",
        "    return {\n",
        "        \"accuracy\": metrics.accuracy_score(targets, predictions),\n",
        "        \"balanced_accuracy\": metrics.balanced_accuracy_score(targets, predictions),\n",
        "        \"precision\": metrics.precision_score(targets, predictions, average=\"weighted\"),\n",
        "        \"recall\": metrics.recall_score(targets, predictions, average=\"weighted\"),\n",
        "        \"f1_score\": metrics.f1_score(targets, predictions, average=\"weighted\")\n",
        "    }"
      ],
      "metadata": {
        "id": "13FaQlXcNvM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Hyperparameters\n",
        "input_size = 32*32*3 # 32x32 RGB images\n",
        "num_classes = 10\n",
        "\n",
        "learning_rate = 0.001\n",
        "num_epochs = 100\n",
        "batch_size = 16\n",
        "activation_function = nn.ReLU()\n",
        "\n",
        "loss_function = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "tyat2KDLL92I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Loaders\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "OYk-5ANCMIRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Training loop\n",
        "\n",
        "# Build the model\n",
        "mlp = MLP(input_size=input_size, num_classes=num_classes, activation_function=activation_function)\n",
        "mlp.cuda()\n",
        "\n",
        "# Setting optimizer up\n",
        "optimizer = torch.optim.Adam(mlp.parameters(), lr=learning_rate)\n",
        "\n",
        "# Early stopping setup\n",
        "best_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "# Start training epochs loop\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "  epoch_loss = 0.0\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images = images.view(-1,32*32*3).cuda() # flattenning images\n",
        "    labels = labels.cuda()\n",
        "\n",
        "    # Forward pass\n",
        "    optimizer.zero_grad()\n",
        "    outputs = mlp(images)\n",
        "\n",
        "    # Backward pass\n",
        "    loss = loss_function(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    epoch_loss += loss.item()\n",
        "\n",
        "    if (i+1) % 1000 == 0:\n",
        "      tqdm.write(f' Epoch {epoch + 1}/{num_epochs}, Step {i+1}/{len(train_dataset) // batch_size}, Loss: {loss}')\n",
        "\n",
        "  epoch_loss /= len(train_loader)\n",
        "  tqdm.write(f'Epoch {epoch+1} average loss: {epoch_loss:.4f}')\n",
        "\n",
        "  # Early stopping using loss value\n",
        "  if epoch_loss < best_loss:\n",
        "    best_loss = epoch_loss\n",
        "    patience_counter = 0\n",
        "  else:\n",
        "    patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "      tqdm.write(\"Early stopping triggered.\")\n",
        "      break"
      ],
      "metadata": {
        "id": "XjTJpiboLmSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Evaluate model (accuracy, precision, recall)\n",
        "mlp.eval()\n",
        "predictions = []\n",
        "labels = []\n",
        "for images, label in test_loader:\n",
        "  images = images.view(-1,32*32*3).cuda()\n",
        "  label = label.cuda()\n",
        "\n",
        "  output = mlp(images)\n",
        "  _, predicted = torch.max(output,1)\n",
        "\n",
        "  predictions.extend(predicted.cpu().numpy())\n",
        "  labels.extend(label.cpu().numpy())\n",
        "\n",
        "scores = get_scores(labels, predictions)\n",
        "print(\"Scores of your model\\n\", scores)"
      ],
      "metadata": {
        "id": "jMA-DgwPhkVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# You can change/optimize this as you want\n",
        "- Different optimizers, activation functions, etc\n",
        "- Automatic hyperparameters optimization (Optuna)\n",
        "- Regularization techniques\n",
        "- Validation set to track metrics during epochs\n",
        "- Transform input data\n",
        "- ..."
      ],
      "metadata": {
        "id": "bA8HgFj-MjYc"
      }
    }
  ]
}